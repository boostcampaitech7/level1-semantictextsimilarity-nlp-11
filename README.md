# **문장 간 유사도 측정**

## 개요

1. 과제 소개

   본 프로젝트의 목표는 **STS(Semantic Textual Similarity, 문맥적 유사도 측정)** 태스크를 수행하는 모델을 개발하는 것입니다.

   STS란 <ins>두 문장이 담고 있는 의미가 얼마나 유사한지</ins> 평가하는 작업으로, 본 프로젝트에서는 0 ~ 5 사이의 값으로 점수를 부여합니다. 예를 들어, 아래와 같이 두 문장이 서로 거의 비슷한 의미를 담고 있으면 높은 점수를 부여합니다.

    <div>

   | 문장 1                                                                                    | 문장 2                                                                                 | 유사도 점수 |
   | :---------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- | :---------- |
   | 진짜 최고의 명작이다                                                                      | 역시 여전히 진짜 명작이다.                                                             | 4.0         |
   | 소년법 폐지 절실합니다.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 소년법 폐지 강력하게 청원합니다.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 4.0         |

    </div>

   반면 아래와 같이 두 문장이 서로 다른 의미를 담고 있으면 낮은 점수를 부여합니다. 특히 일부 단어나 문장 구조가 비슷하다고 해도 문장의 의미가 다르면 유사도 점수는 낮게 평가됩니다.

    <div>

   | 문장 1                       | 문장 2                                 | 유사도 점수 |
   | :--------------------------- | :------------------------------------- | :---------- |
   | 그렇게 재미있진 않네요.      | 그런데 많이 재미있었습니다.            | 0.8         |
   | 유치하고 촌스러운 애니메이션 | 유일하게 질리지 않는 (영화)애니메이션. | 0.8         |

    </div>

   이러한 유사도 측정 작업은 번역 품질 평가(원본 문장 - 번역 문장의 유사도 평가), 질의 응답(입력 질문과 기존 질문 리스트 중 유사한 것을 찾아 적절한 답변 제공), 중복 문서 제거 등 자연어 처리의 다양한 분야에 응용될 수 있습니다.
   <br><br>

2. 평가 지표

   $$
   (\text{Pearson Correlation Coefficient})= {{\sum (y - \bar y)(t - \bar t)}\over \sqrt{(\sum{(y - \bar y)^2}} \sqrt{\sum{(t - \bar t)^2}}}
   $$

   본 대회에서는 **피어슨 상관 계수**를 평가 지표로 사용합니다. 피어슨 상관 계수란, 두 변수의 선형 상관 관계를 계량화한 수치로, +1과 -1 사이의 값을 갖습니다. **두 변수가 강한 양의 선형 상관 관계를 가질수록 피어슨 상관 계수는 +1에 가까워지고**, 강한 음의 상관 관계를 가질수록 상관 계수는 -1에 가까워집니다. 본 프로젝트에서는 유사도 점수의 실제 값(label)과 모델의 예측값(target)이 양의 상관 관계를 가질수록 좋은(유사도 예측의 경향이 실제 값의 경향과 일치하는) 모델이 되는 것이므로, 피어슨 상관 계수를 +1에 가깝게 만드는 것이 목표가 됩니다.

   모델A가 모델B보다 피어슨 상관 계수가 높다는 것은, 모델B보다 <ins>모델A의 예측값들의 전체적인 경향(증감율)이 실제 값들의 경향(증감율)과 더 비슷하다</ins>는 것을 의미합니다. 단, 이것이 모델A가 모델B보다 실제 값에 가깝게 예측하는 것을 보장하지는 않으므로 해석에 주의가 필요합니다. (실제 값에 가깝게 예측하는지도 평가하고자 한다면 MSE 등 다른 평가 지표를 함께 사용하는 것이 좋습니다.)

<br/>

## 팀원 소개
| <img alt="강정완" width="140" height="140" src="https://github.com/user-attachments/assets/f46c690d-d838-496c-8f0d-c52285e782b2" /> | <img alt="김민선" width="140" height="140" src="https://github.com/user-attachments/assets/404b7b75-b308-4a50-bbb9-d827f7936d96" /> |<img alt="서선아" width="140" height="140" src="https://github.com/user-attachments/assets/c3e11cb6-dd7d-4803-a8cd-b20231ccb3c7" /> | <img alt="이인구" width="140" height="140" src="https://github.com/user-attachments/assets/acc572e1-a38d-432a-b95d-bf66b588f5bb" /> | <img alt="이재협" width="140" height="140" src="https://github.com/user-attachments/assets/3c5b062e-fa31-4ba0-94b5-47968ec26c28" /> | <img alt="임상엽" width="140" height="140" src="https://github.com/user-attachments/assets/a569834b-fdd4-4065-b341-4c405ee87230" />
|:---:|:---:|:---:|:---:|:---:|:---:|
| **강정완** | **김민선** | **서선아** | **이인구** | **이재협** | **임상엽** |
| 전과정 참여 <br> 프로젝트 템플릿 제작 | 전과정 참여 | 전과정 참여 | 전과정 참여 | 전과정 참여 | 전과정 참여 |
| [GJ98](https://github.com/GJ98) | [CLM-BONNY](https://github.com/CLM-BONNY) | [seon03](https://github.com/seon03) | [inguelee](https://github.com/inguelee) | [jhyeop](https://github.com/jhyeop) | [gityeop](https://github.com/gityeop) | 

<br/>

## 프로젝트 진행
- AI 프로젝트 전체 구조와 흐름에 대한 적응과 학습을 위해 팀원 전원이 End-to-end로 프로젝트 전 과정에 참여했습니다.
<img width="1000" alt="스크린샷 2024-09-30 07 32 00" src="https://github.com/user-attachments/assets/f7d04dea-e572-40ef-9c46-71079dd45e91">

<br/>

## 프로젝트 템플릿
프로젝트를 진행할 때 여러 가지 모델, 전처리 방식, 하이퍼 파라미터 등을 조합하여 다양하게 실험해보는 것이 중요하다고 생각했습니다. [해당 탬플릿](https://github.com/GJ98/pytorch-template)은 모델, 데이터셋, 손실함수 등 학습 구성 요소를 별도 파일로 두어 코드 관리를 용이하게 해주고, 설정 파일 수정만으로 구현 클래스와 하이퍼파라미터를 쉽게 변경할 수 있도록 하여 다양한 실험을 효율적으로 수행할 수 있게 해줍니다.

<br/>

## 데이터 분석
### 1. 데이터 라벨링 기준

**공식 라벨 분류 기준**

- 0점 : 두 문장의 핵심 내용이 동등하지 않고, 부가적인 내용에서도 공통점이 없음
- 1점 : 두 문장의 핵심 내용은 동등하지 않지만, 비슷한 주제를 다루고 있음
- 2점 : 두 문장의 핵심 내용은 동등하지 않지만, 몇 가지 부가적인 내용을 공유함)
- 3점 : 두 문장의 핵심 내용은 대략적으로 동등하지만, 부가적인 내용에 무시하기 어려운 차이가 있음
- 4점 : 두 문장의 핵심 내용이 동등하며, 부가적인 내용에서는 미미한 차이가 있음
- 5점 : 두 문장의 핵심 내용이 동일하며, 부가적인 내용들도 동일함

**라벨 분류 기준 재해석**

- 0점: 중요 단어/문구가 전혀 공유되어 있지 않음
- 1점: 중요 단어/문구 한개만 공유하고 나머지 중요 단어/문구는 서로 무관하거나 상반됨
- 2점: 중요 단어/문구 한개를 공유하고, 나머지 중요 단어/문구는 연관성이 있음
- 3점: 여러 중요 단어/문구를 공유하지만, 부사/부사구가 다름
- 4점: 여러 중요 단어/문구를 공유하고, 부사/부사구까지 동일함
- 5점: 감탄사 및 세부 표현까지 모두 동일함

중요 단어/문구: slack=키워드, petition=정책 대상/요청사항, nsmc=평가 대상/내용

중요 단어/문구가 **연관성**을 가진다 = 의미가 비슷한 중요 단어/문구를 가지고 있다. (e.g., 휴가/주말, 삭감/미지급, *즐길만함/재밌었던*)

*중요 단어/문구를 **공유**한다. = 의미가 (거의) 동일한 중요 단어/문구를 서로 가지고 있다. (e.g., 봬요/만나요,  일회용품/일회용컵, 빵빵 터진다/진짜 재밌다)*

결론: 0점과 5점은 학습이 수월해 보인다. 반면에, 2점과 3점 학습이 어려워 보인다. 연관성이 있는 단어(재밌다/좋다)와 의미가 동일한 단어(봬다/만나다)의 차이를 명확히 알아야 구분할 수 있기 때문이다.

### 2. EDA: train / dev 데이터 셋의 label별 / 소스별 분포
**Train Label Distribution (훈련 데이터 레이블 분포)**
  
   ![train](https://github.com/user-attachments/assets/10653a9c-b001-4658-ae66-ea6e5d40f276)
  
- 설명: 이 그래프는 훈련 데이터셋에 포함된 각 레이블(label)의 분포를 보여줍니다. x축은 레이블의 값(0에서 5까지), y축은 해당 레이블에 속하는 데이터의 개수입니다.
- 관찰 사항: 특정 레이블(0, 2, 3, 5)의 데이터는 매우 많지만, 일부 레이블(1, 4)은 상대적으로 적습니다. 훈련 데이터셋에서 각 레이블이 균형 있게 분포하지 않음을 나타냅니다.

**Dev Label Distribution (검증 데이터 레이블 분포)**
  
   ![dev](https://github.com/user-attachments/assets/5e2475eb-4bbf-451d-9e80-1b69f4306817)
  
- 설명: 이 그래프는 검증(Dev) 데이터셋에서 레이블 분포를 시각화한 것입니다.
- 관찰 사항: 모든 레이블이 비교적 균등하게 분포되어 있습니다. 각 레이블의 개수 차이가 훈련 데이터보다 적고, 레이블의 편차가 덜함을 알 수 있습니다.

**Train Source Distribution (훈련 데이터 출처 분포)**
  
   ![train_source](https://github.com/user-attachments/assets/f1c5c730-6eae-406d-a5d9-bfc2e14627ee)
  
- 설명: 훈련 데이터가 여러 소스(source)에서 수집된 것을 보여주는 그래프입니다. x축은 데이터의 출처(예: NSMC, Slack, Petition 등), y축은 각 출처에서 제공한 데이터 개수입니다.
- 관찰 사항: `nsmc-sampled`, `slack-sampled`과 같은 소스가 대부분의 데이터를 제공하고 있습니다. 반면 `slack-rtt`나 `petition-rtt`와 같은 소스는 상대적으로 적은 데이터를 제공하고 있습니다.

**Dev Source Distribution (검증 데이터 출처 분포)**
  
   ![dev_source](https://github.com/user-attachments/assets/4bf5e467-240f-4b32-944b-e1952366483b)
  
- 설명: 이 그래프는 검증 데이터의 출처 분포를 나타냅니다. 각 출처로부터 얼마나 많은 데이터가 제공되었는지 시각화하고 있습니다.
- 관찰 사항: 검증 데이터도 `nsmc-sampled`과 `petition-sampled`에서 많이 수집되었지만, 훈련 데이터에 비해 출처 간 데이터 분포의 편차가 상대적으로 적습니다.

**종합 설명**

- **훈련 데이터셋**에서는 일부 레이블과 소스가 편중되어 있는 경향이 강합니다. 특히 레이블 0, 2, 3이 많이 존재하고, `nsmc-sampled`과 같은 소스에서 대부분의 데이터를 가져온 것이 눈에 띕니다.
- **검증 데이터셋**은 레이블 분포가 더 균등하고, 데이터 소스도 더 다양하게 분포되어 있어 상대적으로 편향이 덜한 데이터셋입니다.

<br/>

## 사전 학습 모델 선택

1. **snunlp/KR-ELECTRA-discriminator**

   **선택 이유:**

   KR-ELECTRA는 34GB의 다양한 한국어 텍스트(위키피디아 문서, 뉴스 기사, 법률 텍스트, 제품 리뷰 등)를 기반으로 사전 학습되었습니다. 특히, 이 모델은 공식적이고 구조화된 텍스트뿐만 아니라 비공식적이고 다양한 문체의 텍스트에도 좋은 성능을 발휘합니다.
   
   - **petition (국민청원 게시판 제목 데이터):** NSMC나 slack과 비교해 문법적으로 구조화된 문장들을 포함하고 있어 KR-ELECTRA의 사전 학습 데이터와 높은 연관성을 가집니다.
   - **NSMC (네이버 영화 감성 분석 코퍼스):** 제품 리뷰와 유사한 데이터로, KR-ELECTRA가 학습한 리뷰 데이터가 영화 리뷰의 문장 간 의미 관계를 이해하는 데 도움이 된다고 판단하였습니다.
   
   **성능 지표:**
   
   - KorSTS (Spearman Correlation): **85.41**

2. **monologg/koelectra-base-v3-discriminator**

   **선택 이유:**
   
   KoELECTRA-base-v3는 20GB의 확장된 한국어 텍스트(신문, 위키, 나무위키, 메신저, 웹 등)를 포함하여 사전 학습되었습니다. 특히, v3 버전은 다양한 형태의 텍스트를 추가로 학습하여 비공식적이고 구어체 문장 처리에 강점을 가지고 있습니다.
   
   - **petition (국민청원 게시판 제목 데이터):** 신문과 위키 등의 공식적인 문서가 포함된 사전 학습 데이터는 청원 제목의 구조화된 유사도 평가에 유리합니다.
   - **NSMC (네이버 영화 감성 분석 코퍼스):** KoELECTRA-base-v3 사전 학습에 사용된 메신저, 웹 데이터는 영화 리뷰와 같이 다양한 주제와 문제 처리에 강점을 가집니다.
   - **slack (업스테이지 슬랙 데이터):** 메신저와 웹 데이터를 포함한 비공식적이고 대화체 텍스트 학습 덕분에 슬랙 데이터의 대화체 문장 유사도 평가에 높은 성능을 기대할 수 있습니다.
   
   **성능 지표:**
   
   - KorSTS (Spearman Correlation): **85.53**

3. **kakaobank/kf-deberta-base**

   **선택 이유:**
   
   KF-DeBERTa는 DeBERTa-v2 아키텍처를 기반으로 하며, 범용 도메인 말뭉치와 금융 도메인 말뭉치를 함께 학습한 특화된 언어 모델입니다. 이 모델은 금융 관련 문장에서 높은 정확도를 가지지만, 범용 도메인에서도 금융 도메인과 마찬가지로 우수한 성능을 보이기 때문에 선택하게 되었습니다.
   
   - **petition (국민청원 게시판 제목 데이터):** 청원 데이터는 비교적 문법에 따라 작성된 데이터로 범용 도메인 말뭉치 중 문법적으로 구조화된 데이터와 유사하여 좋은 성능을 기대할 수 있다고 생각해 선택하게 되었습니다.
   - **NSMC (네이버 영화 감성 분석 코퍼스):** 범용 도메인 작업에서도 우수한 성능을 발휘하는 KF-DeBERTa는 영화 리뷰 문장 사이의 유사도 평가에 효과적이라고 보았습니다.
   - **slack (업스테이지 슬랙 데이터):** 슬랙의 비공식적이고 대화체 문장은 범용 도메인 말뭉치와의 연관성이 높아, 다양한 문체 처리 성능이 슬랙 데이터의 유사도 평가에 기여할 것이라고 보았습니다.
   
   **성능 지표:**
   
   - KorSTS (Spearman Correlation): **85.99**

4. **team-lucid/deberta-v3-xlarge-korean**

   **선택 이유:**

   team-lucid/deberta-v3-xlarge-korean 모델은 현재 우리팀이 사용한 한국어 모델 중에서 가장 큰 사이즈(1.56 GB)를 가지고 있어서 복잡한 문장의 패턴과 미묘한 의미 차이를 포착하는 데 유리하다고 생각하였습니다. xlarge모델의 정확한 evaluation scores는 적혀있지 않아서 파악이 힘들지만 base 모델의 KorSTS 점수가 준수한 점을 들어서 모델을 선택하게 되었습니다.
   
   성능 지표(DeBERTa-base):
   
   - KorSTS (Spearman Correlation): 84.46

**요약:**

STS 과제의 데이터 소스(petition, NSMC, slack)와 각 모델에서 사전 학습 시 사용된 데이터가 높은 연관성을 가지며, KorSTS 평가에서도 높은 Spearman 상관 계수를 기록한 모델을 선택하였습니다.

- **snunlp/KR-ELECTRA-discriminator**는 다양한 공식 및 비공식 텍스트 처리에 강하며, 안정적인 성능을 보입니다.
- **monologg/koelectra-base-v3-discriminator**는 확장된 데이터로 비공식적 문장 처리에 유리하며, 유사한 높은 성능을 유지합니다.
- **kakaobank/kf-deberta-base**는 범용성과 금융 도메인 특화 능력을 겸비하여, KorSTS 평가에서 가장 높은 상관 계수를 기록함으로써 다양한 주제의 문장 쌍을 효과적으로 평가할 수 있습니다.
- **team-lucid/deberta-v3-xlarge-korean**은 가장 큰 모델 사이즈를 통해 복잡한 문장의 패턴과 미묘한 의미 차이를 효과적으로 포착할 수 있어, 문장 유사도 평가에서 높은 정확도와 정밀도를 기대할 수 있습니다.

<br/>

## 학습 설계 및 실험 결과 분석

   실험 결과 도식은 세 가지 그래프가 한 쌍으로 구성되어 있으며, 각 그래프의 의미는 아래와 같습니다.
   
   - 방법론을 적용한 모델의 예측 값 분포 (label - prediction)
   - 베이스 모델의 예측 값 분포 (label - prediction)
   - error_difference의 분포 (label - error_difference)
       
       $error\ difference = error_{base} - error_{method} \ \ (error_{base} = |prediction_{base} - label|, \ error_{method} = |prediction_{method} - label|)$
       
       - error_difference > 0 이면 방법론을 적용했을 때 예측 값이 베이스 모델의 예측 값보다 label에 더 가까워졌음을 의미합니다.(모델의 예측 정확도 향상에 기여) 반대로 error_difference < 0 이면 방법론을 적용한 예측 값이 베이스 모델의 예측 값보다 label에서 멀어졌음을 의미합니다.(모델의 예측 정확도 하락에 기여) 각 영역에 속하는 데이터 수를 비교하여 모델 성능이 개선되었음을 대략적으로 확인하였습니다.

### 데이터 전처리

1. **맞춤법 교정:** 한국어 맞춤법 교정 라이브러리인 `py-hanspell`을 사용하여 입력 데이터를 전처리 수행
    
    
    |  | **원본** | **py-hanspell 적용** |
    | --- | --- | --- |
    | 띄어쓰기 | 스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~ | 스릴도 있고 반전도 있고 여느 한국 영화 쓰레기들하고는 차원이 다르네요~ |
    | 맞춤법 교정 | 그 책부터 언능 꺼내봐야 겠어요! | 그 책부터 얼른 꺼내봐야겠어요! |
    | 오류 | 이건 진짜 대박임ㅇㅇ | 이건 진짜 대박 임용 |
    |  | 스우파 리정 vs 시미즈에서 리정 배틀곡이기도합니다 | 스투파 리 전 vs 시미즈에서 리 전 배를 곡이기도 합니다 |
    |  | 보육교사대 아동비율수 조정 청원합니다. | 보육교사다 아동 비율 수 조정 청원합니다. |
    |  | 결론은 완전 노잼. | 결론은 완전 나 잼. |
    |  | 와아아아안전 좋아요오오 | 와아아아 안전 좋아요 오 오 |
    |  | 추가로 스우파-스걸파-쇼미를 잇는 너무 즐거운 취향 공유 시간도 너무 즐거웠네요. | 추가로 스 우파-그걸 파-쇼미를 잇는 너무 즐거운 취향 공유 시간도 너무 즐거웠네요. |
    |  | 자유한국당 퇴출 시킨시다. | 자유한국당 퇴출 시킨 시다. |
    
    전반적으로 맞춤법 교정이 잘되고 있으나, 몇가지 케이스에서 원본 데이터 의미의 왜곡이 관찰되었다.
    
    - 실제 실험 결과(그래프)
        - snunlp/KR-ELECTRA-discriminator
        
           ![image](https://github.com/user-attachments/assets/903972b8-6d3f-4756-8e6f-d9a91c1b288b)
        
        - monologg/koelectra-base-v3-discriminator
        
           ![image](https://github.com/user-attachments/assets/72e91c80-fc35-44b8-a716-00d3043c8136)

        
        - kakaobank/kf-deberta-base
        
           ![image](https://github.com/user-attachments/assets/dd1502ab-39b9-4af7-b74b-25e4750cc7c8)

        
        - 해석
            
            그래프를 통해 맞춤법 교정이 일부 영역에서는 성능 향상을 가져오지만, **모든 경우에 긍정적인 결과를 주는 것은 아님**을 알 수 있습니다. 특히 **이모티콘(ㅇㅇ), 고유명사, 신조어, 비표준어, 구어체** 등에서 원본 문장의 의미를 왜곡하는 한계를 보였습니다. 이것은 인터넷에서 수집된 데이터의 특성을 처리하기에 맞춤법 교정이 한계가 있다는 의미입니다. 따라서 데이터의 특성을 고려하지 않은 일률적인 맞춤법 교정은 오히려 모델의 성능을 저해할 수 있습니다. 만약 맞춤법 교정을 사용해야 한다면 맞춤법 교정을 통해 전처리된 데이터를 한번 더 전처리하여 데이터의 특성을 더 잘 표현할 수 있도록 해야합니다.
          
2. **띄어 쓰기 교정**
   - 기대 효과
     
     띄어쓰기가 안 된 데이터로 인해 unknown 토큰이 발생하게 됩니다. 띄어쓰기가 잘못되면 문장을 분석하기 어려워지고, 모델은 해당 단어를 제대로 인식하지 못해 unknown으로 처리할 가능성이 높아집니다. pykospacing과 같은 띄어쓰기 교정을 하면 토큰화가 잘 이루어지고, 이를 통해 unknown 토큰이 발생하는 빈도를 줄여 이러한 문제를 완화할 수 있습니다.
     
   - 실험 결과
      - snunlp/KR-ELECTRA-discriminator
        
        ![Image20240925233250_snu](https://github.com/user-attachments/assets/9c21c120-88e6-496c-9b34-c07b68cc59a7)

      - monologg/koelectra-base-v3-discriminator
  
        ![Image20240925233334_monolog](https://github.com/user-attachments/assets/9d6ff61d-0f20-4cd9-a698-4cf58379c020)

      - kakaobank/kf-deberta-base

        ![Image20240925233347_kakao](https://github.com/user-attachments/assets/e7254161-cc81-4d57-8dd3-6496940aabd4)

   - 해석
     
      - 베이스라인 모델(첫 번째 그래프)과 방법론(pykospacing Spacing) 모델(두 번째 그래프)을 비교해 봤을 때 약간의 차이가 있지만, 분포는 큰 차이가 없다는 것을 볼 수 있으며, 대각선(1:1) 부분에 분포하고 있는 것을 확인할 수 있습니다.
      - 베이스 라인 모델과 방법론 모델의 오차 값(세번째 그래프) 을 계산한 그래프인데 레이블 0에서 Spacing이 Base보다 나은 경우가 43번이고, Base가 나은 경우가 45번으로 비슷합니다. 레이블 1에서 Spacing이 49번 나은 반면, Base는 61번 나은 경우가 있습니다. 즉, Base가 더 나은 결과를 많이 보였습니다. 레이블 2부터 5까지는 Spacing이 더 우수한 경향이 있음을 볼 수 있습니다. 특히 레이블 5에서는 Spacing이 73번, Base가 59번 더 나은 결과를 보였습니다.
      - 결론 : 레이블 1에서는 Base가 우수한 결과를 더 자주 보인 반면, 레이블 2부터 5까지는 Spacing이 전반적으로 더 우수한 성능을 보였습니다. Spacing과 Base 간의 성능 차이가 레이블에 따라 다르게 나타나며, 이를 기반으로 특정 상황에서 어느 방법이 더 적합한지 판단할 수 있습니다.

   
3. **영어 → 한글 변환**: `hangulize`를 이용해 영단어를 한글 발음으로 변환하는 작업을 수행하였습니다.
    - 예시
    
       | **원본** | **hangulize 적용** |
       | --- | --- |
       | 제가 있는 회의실의 **jabra** 마이크/스피커 들고 가신분?? | 제가 있는 회의실의 자바라 마이크/스피커 들고 가신분?? |
  
    - 기대 효과
    
      이 작업으로 기대한 바는 실제로 의미가 유사한 문장들의 유사도가 영단어와 한글 단어로 인해 생기는 차이를 줄여 문장들의 유사도를 좀 더 올바르게 판별해 모델 정확도가 높아지는 것입니다.

    - 실험 결과
      - snunlp/KR-ELECTRA-discriminator

        ![image](https://github.com/user-attachments/assets/7e98125d-8f9f-4a6a-b48a-cbd7f7d5ed3e)
        
      - monologg/koelectra-base-v3-discriminator
   
        ![image](https://github.com/user-attachments/assets/4c344600-c7c5-42fb-8e03-dc10cf3663ec)

      - kakaobank/kf-deberta-base
   
        ![image](https://github.com/user-attachments/assets/b1822634-4787-408a-b19b-2796b3ba4982)

    
     - 해석
           
       베이스 라인 모델의 경우 데이터 포인트들이 대체로 대각선(1:1 선)을 따라 분포하고 있어, 예측값이 실제 값과 가까운 것을 보여줍니다. 방법론 적용 모델의 그래프 역시 데이터 포인트가 대각선 근처에 분포하고 있지만, 약간의 차이가 보입니다. 방법론 적용을 통해 예측 성능에 긍정적인 영향을 미친 것으로 볼 수 있었습니다. 두 모델의 오차를 비교한 그래프의 경우(각 행의 세번째 그래프), 대체로 파란색 점이 주황색 점보다 많지만 전반적으로 점들의 분포가 고르게 퍼져 있습니다. 이는 방법론이 베이스 라인 모델에 비해 긍정적인 영향을 미친 경우가 많지만, 특정 레이블에서는 여전히 성능 저하가 발생할 수 있음을 보여줍니다. 
        
       - 성능 저하 원인 데이터 예시
             
            | **원본** | **hangulize 적용** |
            | --- | --- |
            | **“github co-pilot**은 제가 깜빡하기 쉬운 **param**을 넣어주거나 **if** 같은것의 조건들을 다 체크해줘서 편한것 같습니다.”, “와우 **notion** 참 필요하다 싶은 기능들을 잘 제공해주면서 성장하는 것 같네요… 다른 기능들을 보니,,!” | **“기시브 코필로트**은 제가 깜빡하기 쉬운 **파람**을 넣어주거나 **이브** 같은것의 조건들을 다 체크해줘서 편한것 같습니다.”, “와우 **노티온** 참 필요하다 싶은 기능들을 잘 제공해주면서 성장하는 것 같네요… 다른 기능들을 보니,,!”|

4. **스페셜 토큰 추가**
   - 기대 효과
      - 데이터를 살펴보는 과정에서 소스별 데이터의 특성이 각각 다르다는 것을 확인했습니다. 이를 이용하여 각 소스별 스페셜 토큰을 추가하고 데이터의 특성을 모델이 인식하게 만들면 더 정확한 예측이 가능할 것이라고 기대했습니다.
   - 실험 결과
      - snunlp/KR-ELECTRA-discriminator
           ![snunlp](https://github.com/user-attachments/assets/d77b1d5a-0c4d-496e-8767-96bd7b9fcc4a)

     - monologg/koelectra-base-v3-discriminator
           ![monologg](https://github.com/user-attachments/assets/5eb27192-eab3-42e0-a28d-76ee08e44a82)

      - kakaobank/kf-deberta-base
           ![kakao](https://github.com/user-attachments/assets/0dce888b-83eb-4796-8f7d-a9b488e19b01)

   - 해석
      - 세 모델 모두 스페셜 토큰을 추가했을 때 예측 값 분포가 극적으로 개선되지는 않았으나, val_pearson은 소폭 상승하였습니다. 위의 실험 결과에서 각 모델의 3번쨰 그래프를 살펴보면 4점, 5점 구간 안에 예측이 label에서 멀어진 데이터(주황색)가 가까워진 데이터(파란색)에 비해 약간 더 많은 것을 확인할 수 있습니다. 성능 평가 지표가 pearson 상관계수이기 때문에 예측값에서 멀어진 데이터들이 증감율을 비슷하게 맞춰 val_pearson이 상승했다고 해석했습니다. 

### 데이터 증강

1. **train dataset 전체 대상 swap 증강**
     - 기대 효과
       - `문장1, 문장2, 점수` 와 `문장2, 문장1, 점수` 데이터를 모두 학습함으로써 문장 순서에 관계 없이 유사도를 예측할 수 있는 방향으로 학습이 되기를 기대하였습니다.
     - 실험 결과
       - snunlp/KR-ELECTRA-discriminator
       
          ![fig_snunlp_aug=fullswap](https://github.com/user-attachments/assets/42796f7b-0c93-446a-aa9b-12d2b9ee0bff)
       
       - monologg/koelectra-base-v3-discriminator
       
          ![fig_monologg_aug=fullswap](https://github.com/user-attachments/assets/1bca07a5-af6e-444c-b951-96f736aa8404)
       
       - kakaobank/kf-deberta-base
       
          ![fig_kakaobank_aug=fullswap](https://github.com/user-attachments/assets/4750b015-9e0d-47e4-aa87-914f6a9e14f1)
       
   
     - 해석
       - 세 모델 모두 swap 증강을 적용했을 때 예측 값 분포가 극적으로 개선되지는 않았으나, `val_pearson`은 소폭 상승하였습니다. 각 모델의 3번 그래프에서 예측이 label에 더 가까워진 데이터(파란색)가 멀어진 데이터(주황색)에 비해 약간 더 많은 것을 확인할 수 있습니다. 이는 swap 증강을 적용하여 문장 순서에 관계 없이 유사도를 측정함으로써 성능이 향상된 정도가 swap 증강의 부작용으로 같은 문장이 두 번씩 학습됨으로써 학습 데이터에 과적합 되어 성능이 하락한 정도보다 조금 더 크다고 해석하였습니다.
       실제로 아래 그래프와 같이 원본 dev dataset과 문장 순서를 바꾼 dataset에 대한 예측 값 차이를 살펴보면, 베이스 모델에서보다 증강 데이터로 학습한 모델에서 분산이 줄어들었음을 확인할 수 있습니다. (dev dataset의 문장 순서를 바꾼 데이터는 두 모델 모두 학습에 사용하지 않음) 즉, 증강 데이터로 학습한 모델은 문장 순서가 바뀌어도 비슷한 점수를 예측할 가능성이 높아졌을 것이라고 판단했습니다.
           
           ![pred_diff_dist](https://github.com/user-attachments/assets/42813cb7-69ef-4ff9-8eac-850853d71430)
        
2. **train dataset의 0.5~3.5점인 데이터 대상 swap 증강**
    - 기대 효과
       - 데이터 분석 결과, 2\~3점 구간의 데이터 학습이 어려울 것이라 판단했고, 실제로 해당 구간에서 예측이 안되는 모습을 관찰할 수 있습니다. 이를 개선하기 위해 0.5\~3.5점 구간의 데이터를 swap 증강을 하여 학습하면 2~3점 구간에서의 예측 오차를 감소시킬 수 있다고 판단했습니다.
    - 실험 결과
        - snunlp/KR-ELECTRA-discriminator
  
           ![fig_snunlp_aug=0535swap](https://github.com/user-attachments/assets/393a0760-61f8-4ec5-8b2f-fce6239880a2)
        
        - monologg/koelectra-base-v3-discriminator
  
           ![fig_monologg_aug=0535swap](https://github.com/user-attachments/assets/1a4af4c3-a700-41fb-899a-eb7a0cc9d2b5)
        
        - kakaobank/kf-deberta-base
  
           ![fig_kakaobank_aug=0535swap](https://github.com/user-attachments/assets/bec4dccd-2cb5-4214-836e-c143b367f6e3)
        
   - 해석
      - 0.5\~3.5점 구간의 데이터 swap 증강을 적용해본 결과, 세 모델 모두 2\~3점 구간의 데이터에서 총 예측 오차를 줄였습니다.

         |  | snunlp | monologg | kakao |
         | --- | --- | --- | --- |
         | swap 증강 모델 총 예측 오차 | 71.3008(-3.8369) | 69.9008(-7.0310) | 74.7366(-7.5463) |
         | baseline 모델 총 예측 오차 | 75.1377 | 76.9318 | 82.2829 |

      - 물론, 해당 구간의 데이터 중 과반수 이상이 라벨값에 가까워졌지만, 멀어진 경우도 있습니다. 예측값이 라벨값에 더 가까워진 이유는 더 많은 데이터로 해당 구간을 학습했기 때문입니다. 반대로, 예측값이 라벨값에 더 멀어진 이유는 증강 데이터가 기존 데이터와 거의 동일한 데이터이기 때문에 모델이 증강 데이터를 학습할 때 기존 데이터에 대한 패턴을 과하게 학습하게 되어, 괴적합 현상이 발생했다고 해석할 수 있습니다.

         |  | snunlp | monologg | kakaobank |
         | --- | --- | --- | --- |
         | 예측값이 라벨값에 가까워진 경우 | 70(+30) | 70(+30) | 60(+10) |
         | 예측값이 라벨값에 멀어진 경우 | 40 | 40 | 50 |
  
      - 결론, 0.5\~3.5 구간의 데이터 swap 증강을 적용하면 학습 데이터의 특정 부분에서 과적합이 발생할 수 있지만, 기존에 학습이 어려웠던 부분인 2\~3점 구간에서의 성능 향상을 더 크게 기대할 수 있습니다.


### 모델링

- segment embedding: 모델링 적용 방식에 따라 각 문장 토큰에 0, 1(스페셜 토큰 추가의 경우 2까지)를 할당해서 문장을 구분할 수 있도록 했습니다.
- 소스별 특화 모델 학습 (rtt, sampled / nsmc, petition, slack): 소스별로 데이터를 분리하여 해당 소스에 특화된 모델을 학습한 후, 결과를 병합했습니다.
- 이진 분류(0\~3→0 / 3\~5→1) 후 점수 예측 모델: 0\~3 구간의 데이터는 0, 3\~5 구간의 데이터는 1로 분류하는 이진 분류 모델을 통과한 다음, 각 집합의 데이터를 별도의 모델로 학습하는 방식을 구현했습니다.

<br/>

## 최종 제출 결과

- 위 실험 분석 결과와 `val_pearson` 점수를 토대로 모델을 선정하고, soft voting 방식의 앙상블 기법을 적용하여 테스트 데이터 셋에 대한 아웃풋을 산출하여 리더보드에 제출하였습니다. 앙상블 기법을 적용하면 단일 모델만 사용하였을 때보다 테스트 데이터 셋에 대한 피어슨 상관 계수가 크게 증가하는 모습을 볼 수 있었습니다.

- 앙상블에 사용한 모델 목록
    
    |  | 베이스 모델 | 학습 데이터 | val_pearson |
    | --- | --- | --- | --- |
    | 모델 1 | kakaobank/kf-deberta-base | 증강 X | 0.9291 |
    | 모델 2 | monologg/koelectra-base-v3-discriminator | 0.5 ~ 3.5점만 swap 증강 | 0.9311 |
    | 모델 3 | snunlp/KR-ELECTRA-discriminator | 0 ~ 5점 전체 swap 증강, 스페셜 토큰 추가 | 0.9369 |
    | 모델 4 | team-lucid/deberta-v3-xlarge-korean | 0 ~ 5점 전체 swap 증강 | 0.9427 |
    | 모델 5 | snunlp/KR-ELECTRA-discriminator | 0 ~ 5점 전체 swap 증강, 스페셜 토큰 추가 | 0.9388 |
  
- 제출 결과
    
    |  | 모델 1 | 모델 2 | 모델 3 | 모델 4 | 모델 5 | pearson_public | pearson_private |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | 앙상블 1 | 0.9291 / total | 0.9311 / total | 0.9369 / total | 0.9427 / total | X | 0.9352 | 0.9358 |
    | 앙상블 2 | 0.1 | 0.2 | 0.3 | 0.4 | X | 0.9367 | 0.9360 |
    | 앙상블 3 | 0.1 | 0.2 | 0.15 | 0.4 | 0.15 | 0.9370 | 0.9364 |
    | 앙상블 4 | 0.1 | 0.15 | 0.15 | 0.45 | 0.15 | 0.9370 | 0.9361 |

